# Probability :
    It is about determining the likelihood of an event.
    
    Example -- Tossing a coin and possibility of getting a head is 50%.

### Mutual Exclusive Events :
    2 Events are mutual exclusive if they cannot occur at the same time.

    Example : tossing a coin -- we can not get `head` and `tail` at the **same** time.

### Non-Mutual Exclusive Events :
    2 Events are non-mutual exclusive if they can occur at the same time.

    Example : Deck of cards -- we can get a `k` and a `heart` at the same time.

## Addition Rule :

- Addition Rule for mutual exclusive event :

![addition rule 1](assets/image14.png)

- Addition Rule for non-mutual exclusive event :

![addition rule 2](assets/image15.png)

## Multiplication Rule :

### Independent Events :
    2 Events are independent if their occurrence do not affect each other.

    Example -- Tossing a coin, Rolling a dice

### Dependent Events :
    2 Events are dependent if their occurence directly affect each other.

    Example : Take out a king card from the deck and after that, we get a queen card at the deck.

- Multiplication rule for independent event :

![Multiplication rule 1](assets/image16.png)

- Multiplication rule for dependent event :
    - idea towards Bayes theorem (conditional probability)

![multiplication rule 2](assets/image17.png)

# Probability Distribution Functions :
    Probability distribution function describe how the probabilities are distributed over the values of a random variable.

![pdf](assets/image18.png)

### 3 main types of probability distribution function :
    1. PMF -- Probability Mass Function
    2. PDF -- Probability Density Function
    3. CDF -- Cumulative Distribution Function

### 1. PMF :
    Used for Discrete random variable.

    Example - Rolling a dice (fair dice)

![pmf](assets/image19.png)

### 2. PDF :
    Used for continuous random variables

    It is the gradient (derivative) / slope of the cumulative curve.

![pdf](assets/image20.png)

#### Properties of PDF :
    1. It is non-negative `f(x) >= 0` for all values
    2. Total area under the curve is equal to 1

## Types of Probabilty Distributions :
    1. Bernoulli Distribution -> (pmf)
    2. Binomial distribution -> (pmf)
    3. Normal/ guassian distribution -> (pdf)
    4. Poisson Distribution -> (pmf)
    5. Log Normal distribution -> (pdf)
    6. Uniform distribution -> (pmf)
    7. Power law distribution
    8. Pareto distribution

- As a data scientist, you will be given a dataset abt house price, number of rooms, location of the house, floor, sea side. 
- Some of these features are discrete, whereas some will be continuous.
- So, based on different distributions, we can make a lot of assumptions which will fuel `EDA` and `Feature Engg`.

---

### 1. Bernoulli distribution
    The bernoulli distribution is the simplest discrete probability distribution.

    Represents probability distribution of a random variablethat has exactly 2 possible outcomes : [Success, Failure].

    It is used to model binary outcomes such as :
        - coin flip
        - yes/no question
        - whether student will pass/fail

- There are basically 2 parameters : `p` & `q` where `0 <= p <= 1` and `q = 1 - p`
![Parameters](assets/image21.png)

- PMF : 
    - Consider a company launches a new smartphone `A` 
    - Use => `60%`
    - Not Use => `40%`

    ![COMPANY](assets/image22.png)

- Mathematical notation of PMF : 

![PMF](assets/image23.png)

![pmf 2](assets/image24.png)

- Mean of bernoulli distribution :
    - expected mean we generally get as the probability of k=1

![mean](assets/image25.png)

- Median of Bernoulli distribution :

![median](assets/image26.png)

- Mode of Bernoulli distribution :

![mode](assets/image27.png)

- Variance :
![variance](assets/image28.png)

### 2. Binomial Distribution :
    It is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes-no question 

![alt text](assets/image29.png)

![alt text](assets/image30.png)

![alt text](assets/image31.png)

### 3. Poisson Distribution :

![poisson](assets/image32.png)

# ğŸ“˜ Lesson 1: Bernoulli Distribution

(The foundation of almost everything in probability & ML)

### 1ï¸âƒ£ What problem does Bernoulli solve?
    Bernoulli answers the simplest possible probabilistic question:
    Does an event happen or not?

- There are only two outcomes:
    - Success (1)
    - Failure (0)
- No middle ground.

### Real-world examples
    Email is spam (1) or not spam (0)

    User clicks an ad (1) or doesnâ€™t click (0)

    Loan defaults (1) or doesnâ€™t default (0)

    Machine fails today (1) or works fine (0)

> If your outcome can be encoded as `yes/no`, `true/false`, `1/0` â†’ **Bernoulli** is involved.

## 2ï¸âƒ£ Formal definition (with formula)

A random variable X follows a Bernoulli distribution if:
```
X âˆˆ {0,1}
```
with probability:
```
P(X=1)=p
P(X=0)=1âˆ’p
```

We write:
`Xâˆ¼Bernoulli(p)`

- Key parameters :
    - p = probability of success
    - There is only one parameter

### 3ï¸âƒ£ Mean & Variance (important for ML intuition)
#### Mean (Expected Value)
    E[X]=p

    ğŸ‘‰ Interpretation:
    If you repeat the experiment many times, the average outcome tends to p.

- Example:
    - If p = 0.8, then over many trials, ~80% outcomes are 1.

#### Variance
    Var(X)=p(1âˆ’p)

ğŸ‘‰ **Insights**:

> Maximum variance occurs at p = 0.5

> If p is near 0 or 1 â†’ outcomes are predictable

> This idea shows up directly in classification confidence

### 4ï¸âƒ£ How does the graph look? 
    The x-axis has only two points: 0 and 1

    At x = 1, height = p

    At x = 0, height = 1 âˆ’ p

> ğŸ“Œ There is no curve â€” just two bars

<img width="574" height="381" alt="image" src="https://github.com/user-attachments/assets/9edfa4dd-7f12-41e1-8d08-09434286450d" />


### 5ï¸âƒ£ Why Bernoulli matters for problem solving

    Ask yourself:

    â€œIs this a single decision with two outcomes?â€

    If yes â†’ start with Bernoulli

- This helps you:
    - Choose the right loss function
    - Interpret model outputs correctly- 
    - Understand uncertainty

### 6ï¸âƒ£ Bernoulli in Data Science (Real usage)
#### ğŸ“Š Data Analysis (EDA)

    Binary columns: is_fraud, clicked, passed_exam

    You estimate p as:
    ğ‘ = number of 1s / total samples
	â€‹

#### ğŸ¤– Machine Learning

    Bernoulli is the heart of binary classification.

    Example: Logistic Regression

    Model outputs P(Y=1 | X) â†’ thatâ€™s p

    Target variable follows a Bernoulli distribution

- Thatâ€™s why:
We use Binary Cross-Entropy loss
Predictions are probabilities, not hard labels

#### ğŸ§  Decision Making

    Suppose:
    Probability of churn = 0.7
    Cost of retention = â‚¹100
    Cost of losing user = â‚¹1000
    Bernoulli helps quantify:
    Expected los
    Expected gain
    Whether an action is worth it

#### 7ï¸âƒ£ Common beginner mistake (important âš ï¸)

    âŒ Treating Bernoulli data as continuous
    âŒ Assuming â€œaverage = outcomeâ€
    âŒ Forgetting probabilities â‰  certainty

- Example:
    - Model predicts 0.9
    - Outcome can still be 0
    - Probability â‰  guarantee.

### 8ï¸âƒ£ Bridge to next distribution

- Bernoulli answers:
    - â€œWhat happens in one trial?â€

- Next logical question:
    - â€œWhat happens if I repeat this experiment n times?â€

> â¡ï¸ That leads directly to Binomial Distribution ğŸ¯

## âœ… Summary (lock this in)
    Bernoulli = single yes/no experiment
    Parameter: p
    Mean = p, Variance = p(1âˆ’p)
    Foundation of binary classification
    Used everywhere in ML, analytics, and decision systems

# ğŸ“˜ Lesson 2: Binomial Distribution
    (Counting how many successes happen)

## 1ï¸âƒ£ What problem does Binomial solve?
    Binomial answers this question:
    If I repeat the same yes/no experiment n times, how many successes will I get?

    Key idea:
    Bernoulli â†’ one trial
    Binomial â†’ many identical Bernoulli trials

## 2ï¸âƒ£ Conditions for Binomial (very important âš ï¸)
    A problem is Binomial only if all these are true:
        - Fixed number of trials (n)
        - Each trial has two outcomes (success/failure)
        - Probability of success (p) is constant
        - Trials are independent
    
    If even one condition fails, Binomial is wrong.

## 3ï¸âƒ£ Real-world examples
    Number of users who click an ad out of 100 shown
    Number of defective items in a batch of 50
    Number of heads in 10 coin tosses
    Number of students who pass an exam out of 40

## 4ï¸âƒ£ Formal definition & formula
Let:

X = number of successes

n = number of trials

p = probability of success per trial

Xâˆ¼Binomial(n,p)

Probability Mass Function (PMF)
```
P(X=k) = (kCn)p^k.(1âˆ’p)^nâˆ’k
```
### What each term means (intuition)
    (ğ‘›Cğ‘˜) : Number of ways to choose k successes from n trials

    ğ‘^ğ‘˜ : Probability of k successes

    (1âˆ’p)^nâˆ’k : Probability of failures

    ğŸ‘‰ Binomial = combinatorics + probability

## 5ï¸âƒ£ Mean & Variance (pattern recognition)
    Mean : E[X]=np

    Interpretation:
    Expected number of successes in n trials

### Variance
    Var(X) = np(1âˆ’p)

    Insights:
        - Variability increases with n
        - Same uncertainty structure as Bernoulli, just scaled

## 6ï¸âƒ£ How does the graph behave?
    X-axis: number of successes (0 to n)
    Y-axis: probability
    Shape depends on p

    Cases:
        p = 0.5 â†’ symmetric bell-like shape
        p < 0.5 â†’ skewed right
        p > 0.5 â†’ skewed left
        As n increases, the distribution:
        Becomes smoother
        Starts looking Normal (important later!)

## 7ï¸âƒ£ Why Binomial improves problem solving

    Ask:
    â€œAm I counting how many times something happens?â€

    If yes â†’ Binomial

- Examples:
    - â€œHow many will click?â€
    - â€œHow many will fail?â€
    - â€œHow many will convert?â€

## 8ï¸âƒ£ Binomial in Data Science
### ğŸ“Š Exploratory Data Analysis

    Conversion rate analysis
    A/B testing
    Survey responses

- Example:
    - 200 users shown an ad
    - 40 click â†’ estimate p = 0.2
    - Binomial models uncertainty in that estimate

### ğŸ¤– Machine Learning

    Loss functions for classification
    Evaluation metrics like accuracy are binomial-based
    Logistic regression predictions aggregated over samples

### ğŸ§  Decision Making

    Example:

    Expected number of failures = np
    Helps plan inventory, staffing, or risk buffers

### 9ï¸âƒ£ Common mistakes âš ï¸

    âŒ Using Binomial when trials arenâ€™t independent
    âŒ Changing p mid-experiment
    âŒ Using Binomial for time-based events (thatâ€™s Poisson!)

## ğŸ”— Bridge to next distribution

- Binomial assumes:
    - Fixed number of trials

- Next question:
> â€œWhat if events happen over time or space, not in fixed trials?â€

> â¡ï¸ That leads to **Poisson Distribution** â±ï¸

## âœ… Quick summary

    Binomial = count of successes

    Parameters: n, p

    Mean = np

    Used heavily in A/B testing, classification metrics

    Bridge between Bernoulli and Normal


# ğŸ“˜ Lesson 3: **Poisson Distribution**

*(Modeling how often events happen)*

## 1ï¸âƒ£ What problem does Poisson solve?

Poisson answers this question:

> **How many times does an event occur in a fixed interval of time or space?**

Key difference from Binomial:

* **Binomial** â†’ fixed number of trials
* **Poisson** â†’ events happen **randomly**, we donâ€™t know how many trials exist


## 2ï¸âƒ£ Real-world examples

    Number of customer arrivals per hour
    Number of server requests per minute
    Number of accidents per day at an intersection
    Number of emails received per hour
    Number of defects per meter of fabric

These are **rate-based phenomena**.


## 3ï¸âƒ£ Core assumption (very important âš ï¸)

Poisson assumes:

1. Events occur **independently**
2. Events occur at a **constant average rate**
3. Two events **cannot happen at the exact same instant**

If these fail â†’ Poisson fails.


## 4ï¸âƒ£ Formal definition & formula

Let:

* **X** = number of events
* **Î» (lambda)** = average number of events per interval

### Probability Mass Function
![pmf](assets/image33.png)
### Intuition behind the formula

    (e^Î») : probability of **no events**
    (Î»^k) : scales with expected rate
    (k!) : adjusts for ordering

    You donâ€™t memorize this â€” you **understand when to use it**.

## 5ï¸âƒ£ Mean & Variance (unique property)

`mean = variance = Î»`

ğŸ”¥ This is special:

* Mean = Variance
* If variance â‰« mean â†’ Poisson is probably wrong

Data scientists use this to **diagnose models**.

## 6ï¸âƒ£ How does the graph behave?

![Image](https://www.itl.nist.gov/div898/handbook/eda/section3/gif/poipdf4.gif)

![Image](https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2021/08/Poisson_distribution_example.png?fit=576%2C384\&ssl=1)

### Verbal visualization

    X-axis: 0, 1, 2, 3, ...
    Y-axis: probability
    Shape depends on Î»

#### Behavior:

* Small Î» (e.g., 1): right-skewed, peak near 0
* Medium Î» (e.g., 5): skew reduces
* Large Î» (>10): looks **almost normal**

ğŸ“Œ This explains why **Normal distribution appears everywhere**


## 7ï¸âƒ£ Poisson vs Binomial (important contrast)

| Feature    | Binomial | Poisson |
| ---------- | -------- | ------- |
| Trials     | Fixed    | Unknown |
| Time-based | âŒ        | âœ…       |
| Parameter  | n, p     | Î»       |
| Mean       | np       | Î»       |

---

## 8ï¸âƒ£ Poisson in Data Science

### ğŸ“Š Data Analysis

    Traffic analysis
    Call center modeling
    Anomaly detection (sudden spike in Î»)


### ğŸ¤– Machine Learning

    Poisson regression (for count prediction)
    Modeling clicks, views, failures
    Used in recommender systems & forecasting

### ğŸ§  Decision Making
    Example:
    - Avg server requests = 100/min
    - Whatâ€™s the probability of >150 requests?
    - Helps plan **capacity & scaling**

## 9ï¸âƒ£ Common mistakes âš ï¸

    âŒ Using Poisson when rate changes over time
    âŒ Ignoring bursty behavior
    âŒ Using it for proportions instead of counts

## ğŸ”— Bridge to next distribution

Poisson counts **events**.

Next question:

> â€œWhat if measurements cluster naturally around an average?â€

â¡ï¸ **Normal (Gaussian) Distribution** ğŸ””
(the backbone of statistics & ML)

## âœ… Summary

    Poisson = event counts per interval
    Parameter: Î»
    Mean = Variance = Î»
    Used in traffic, failures, demand forecasting


# ğŸ“˜ Lesson 4: **Normal (Gaussian) Distribution**

*(Why â€œaverageâ€ dominates the real world)*

## 1ï¸âƒ£ What problem does Normal distribution solve?

    Normal answers:
    How do values naturally vary around an average?

    Whenever:
    Many small independent factors influence a value
    No single factor dominates

    ğŸ‘‰ The result tends to be **Normal**

This is not magic â€” itâ€™s the **Central Limit Theorem** working silently.
## 2ï¸âƒ£ Real-world examples

* Heights of people
* Exam scores
* Measurement errors
* Sensor noise
* Model prediction errors
* Daily temperature variations

If youâ€™ve ever heard:

> â€œMost values are near the meanâ€

Youâ€™re thinking **Normal distribution**.

## 3ï¸âƒ£ Formal definition & formula

### Probability Density Function (PDF)

![pdf](assets/image34.png)

### What parameters mean

    Î¼ (mean) â†’ center
    Ïƒ (standard deviation) â†’ spread

## 4ï¸âƒ£ Mean, Variance & symmetry

    Mean = Median = Mode = Î¼
    Variance = ÏƒÂ²
    Perfectly symmetric around Î¼

No skew. No heavy tails.

## 5ï¸âƒ£ How does the graph behave?

![Image](https://images.openai.com/static-rsc-3/8ZuyCjkffHxfMnAkUYnsKi1w9kqNS5wLvjHLdcsLve3_dANojPQ7G5ZgvLAWrm3rKvGswUuxG4lbJrfYBQBVPxsQ_xbsvK2_jhvRaI8RwO8)

![Image](https://images.openai.com/static-rsc-3/eP9_AOgsg8i9c9WTpIKRizKjLO961YWYqdzOwRw8Bz5Heq5DNSxZWj5o8DbTj-dxxI-oTPi9v-6-6oMCNg6NocaD8v76zIqE67ykRE_wdBU)

### Verbal visualization

    Bell-shaped curve
    Peak at Î¼
    Symmetric on both sides

### Empirical Rule (68â€“95â€“99.7)

    68% within Â±1Ïƒ
    95% within Â±2Ïƒ
    99.7% within Â±3Ïƒ

This helps with **quick probability estimation**.

## 6ï¸âƒ£ Why Normal improves problem solving

### Mental shortcut

    Ask:
    â€œIs this caused by many small independent effects?â€

    If yes â†’ try Normal.

## 7ï¸âƒ£ Normal in Data Science

### ğŸ“Š Data Analysis

    Z-scores for outlier detection
    Confidence intervals
    Hypothesis testing


### ğŸ¤– Machine Learning

    Gaussian Naive Bayes
    Assumptions behind linear regression
    Weight initialization
    Noise modeling


### ğŸ§  Decision Making

    Example:

    Predict delivery time = 30 Â± 5 minutes
    Probability of delay > 40 mins?

Normal lets you **quantify risk**.


## 8ï¸âƒ£ Common mistakes âš ï¸

    âŒ Assuming Normal without checking
    âŒ Ignoring skew/heavy tails
    âŒ Applying to bounded data (like probabilities)

## ğŸ”— Bridge to next distribution

Normal handles **symmetric variation**.

Next question:
> â€œWhat if data is always positive and skewed right?â€

â¡ï¸ **Log-Normal Distribution** ğŸ“ˆ

### âœ… Summary

    Normal = natural variability
    Parameters: Î¼, Ïƒ
    Backbone of statistics & ML
    Used everywhere

## (Standard Normal & Z-Score)

## 4ï¸âƒ£.9 Standard Normal Distribution

The **Standard Normal Distribution** is just a **special Normal distribution**:
Where:

* Mean = **0**
* Standard deviation = **1**

### Why do we need it?

Because:

* Every Normal distribution has **different Î¼ and Ïƒ**
* We want **one universal reference distribution**

So we **convert any Normal variable into Z**

---

## 4ï¸âƒ£.10 Z-Score (Extremely important)

### Formula

![alt text](assets/image41.png)

### Interpretation

Z-score answers:

> **How many standard deviations away from the mean is this value?**

---

### Example (real-world)

* Mean exam score = 70
* Std dev = 10
* Student score = 85

![alt text](assets/image40.png)

ğŸ‘‰ The student is **1.5Ïƒ above average**

---

## Why Z-scores matter in Data Science

### ğŸ“Š Data Analysis

* Outlier detection
* Feature scaling
* Comparing different metrics on the same scale

### ğŸ¤– Machine Learning

* Gradient descent converges faster
* Models assume normalized inputs
* Distance-based models (KNN, SVM)

### ğŸ§  Decision Making

* Risk assessment
* Performance benchmarking
* Anomaly detection

---

### Mental model

* Raw value â†’ meaningless alone
* Z-score â†’ **context-aware**

---

## âš ï¸ Common mistake

âŒ Applying Z-score when data is **not Normal**

---

Now letâ€™s continue the course.

---

# ğŸ“˜ Lesson 5: **Log-Normal Distribution**

*(Normal â€” but in log space)*

---

## 1ï¸âƒ£ What problem does Log-Normal solve?

Log-Normal answers:

> **What if values are always positive and grow multiplicatively?**

![log normal](assets/image39.png)

## 2ï¸âƒ£ Real-world examples

* Income & wealth
* Stock prices
* File sizes
* Session durations
* Response times
* Product sales

These **cannot be negative** and often have **long right tails**

## 3ï¸âƒ£ Probability density function

![alt text](assets/image38.png)

You donâ€™t memorize this â€” you recognize **when to use it**.

## 4ï¸âƒ£ How does the graph behave?

![Image](https://www.investopedia.com/thmb/dmWOsjhPLEXqOnOFh7_0v3E4wUs%3D/1500x0/filters%3Ano_upscale%28%29%3Amax_bytes%28150000%29%3Astrip_icc%28%29/dotdash_Final_Log_Normal_Distribution_Nov_2020-01-fa015519559f4b128fef786c51841fb9.jpg)

![Image](https://www.researchgate.net/publication/327664780/figure/fig2/AS%3A680230062678016%401539190893047/a-Bars-histogram-of-the-log-normal-distribution-log-s-N-m-2-with-m-057-h-s.ppm)

### Verbal visualization

* Starts near zero
* Long right tail
* Strong right skew
* Mean > Median > Mode

## 5ï¸âƒ£ Why Log-Normal improves problem solving

### Ask yourself:

> â€œCan this value be negative?â€
> â€œDoes growth compound?â€

If yes â†’ **Log-Normal**

## 6ï¸âƒ£ Data Science usage

### ğŸ“Š EDA

* Transform skewed data using log
* Stabilize variance

### ğŸ¤– ML

* Regression on log-transformed targets
* Financial modeling

### ğŸ§  Decision Making

* Revenue forecasting
* Risk modeling

## âš ï¸ Common mistake

âŒ Treating log-normal data as Normal

---

# ğŸ“˜ Lesson 6: **Uniform Distribution**

*(When everything is equally likely)*

## Definition

![formula](assets/image37.png)

## Real-world examples

* Random number generators
* Sampling
* Shuffling data
* Simulation baselines

## Graph behavior

![Image](https://www.itl.nist.gov/div898/handbook/eda/section3/gif/unicdf.gif)

![Image](https://deepdatascience.wordpress.com/wp-content/uploads/2017/03/uniform-vs-normal-distribution.png)

* Flat rectangle
* No preference
* Constant probability

## Usage in Data Science

* Baseline modeling
* Random initialization
* Monte Carlo simulations

---

# ğŸ“˜ Lesson 7: **Power Law Distribution**

*(Few dominate, many are small)*

## Core idea

[
P(X = x) \propto x^{-\alpha}
]

## Real-world examples

* City sizes
* Website traffic
* Social media followers
* Earthquakes

## Graph behavior

![Image](https://mathinsight.org/media/image/image/power_law_degree_distribution_scatter.png)

![Image](https://www.statisticshowto.com/wp-content/uploads/2016/05/heavy-tailed.png)

* Heavy tail
* Scale-free
* No typical average

## Data Science usage

* Network analysis
* Fraud detection
* Rare event modeling

# ğŸ“˜ Lesson 8: **Pareto Distribution**

*(Concrete form of Power Law)*

## Definition

![alt text](assets/image36.png)

## Real-world examples

* 80â€“20 rule
* Wealth distribution
* Bug severity

## Graph behavior

![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Probability_density_function_of_Pareto_distribution.svg/330px-Probability_density_function_of_Pareto_distribution.svg.png)

![Image](https://www.networkpages.nl/wp-content/uploads/2024/05/Figure2.png)


## Usage

* Business optimization
* Risk concentration
* Resource allocation

# ğŸ§  CAPSTONE: Central Limit Theorem (CLT)


## Statement (in plain English)

> **The average of many independent samples tends to be Normal â€” regardless of the original distribution**


## Why CLT is revolutionary

* Justifies Normal assumptions
* Enables confidence intervals
* Enables hypothesis testing
* Powers most statistics & ML

## Visual intuition

* Raw data â†’ any shape
* Sample means â†’ Normal

## Data Science implications

### ğŸ“Š Estimation

![formula](assets/image35.png)

* Larger sample â†’ tighter estimates
* Uncertainty shrinks with data

### ğŸ¤– ML

    Loss convergence
    Model evaluation
    Generalization understanding

### ğŸ§  Decision Making

    Forecast confidence
    Risk intervals
    A/B testing

# ğŸ§© Final Mental Map (Extremely Important)

| Problem Type      | Distribution       |
| ----------------- | ------------------ |
| Yes/No            | Bernoulli          |
| Count successes   | Binomial           |
| Events per time   | Poisson            |
| Natural variation | Normal             |
| Compare values    | Z-score            |
| Positive skew     | Log-Normal         |
| Equal randomness  | Uniform            |
| Extreme dominance | Power Law / Pareto |

